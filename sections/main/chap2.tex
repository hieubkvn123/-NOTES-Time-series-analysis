\newpage
\section{Stationary time-series}
\subsection{Strict vs. Weak stationarity}

\begin{definition}[Mean, covariance and variance functions]
    Given a time-series $(X_t)_{t\in T}$ (where $T$ is a time domain), we have the following functions:
    \begin{itemize}
        \item Mean : $\mu(t)=\mathbb{E}[X_t], \ t\in T$.
        \item Covariance : $\gamma(t, s) = Cov(X_t, X_s), \ t, s\in T$.
        \item Variance : $\sigma^2(t) = \gamma(t,t)= Var(X_t), \ t\in T$.
    \end{itemize}
\end{definition}


\begin{definition}[Strict stationarity]
    A time-series is said to be \textbf{strictly stationary} if, $\forall n \in \mathbb{Z}_+, h \in \mathbb{N}$, we have:
    \begin{align*}
        (X_1, \dots, X_n) \stackrel{d}{\simeq} (X_{1+h}, \dots, X_{n+h}) 
    \end{align*}

    \noindent Meaning the two tuples are equal in distribution.
\end{definition}

\noindent \textbf{Remark} : Note that in practice, strict stationarity is often hard to achieve. Hence, we have some relaxed conditions for stationarity.

\begin{definition}[Weak stationarity]
    A time-series $(X_t)_{t\ge0}$ is said to be \textbf{weakly stationary} if the following two conditions are met:
    \begin{itemize}
        \item $\mu(t)$ does not depend on $t$.
        \item $\gamma(t, h)$ does not depend on $t$ for all $h\in\mathbb{N}$.
    \end{itemize}
\end{definition}

\subsection{Moving Average - $\bf MA(q)$ model}
\begin{definition}[White noise]
    A process $(Z_t)_{t\in T}$ is called \textbf{white noise} if it satisfies:
    \begin{itemize}
        \item $Cov(Z_t, Z_s)$ for all $s\ne t$.
        \item $Var(Z_t) = \sigma^2, \ \forall t\in T$.
    \end{itemize}

    \noindent We denote that $Z_t\sim WN(0, \sigma^2)$.
\end{definition}

\begin{definition}[$\bf MA(q)$ model]
    The moving average of order $q$ is defined as followed:
    \begin{align*}
        X_t = Z_t + \sum_{j=1}^q \theta_j Z_{t-j}, \ Z_t\sim WN(0,\sigma^2)
    \end{align*}
\end{definition}

\textbf{Remark} : The following propositions are proven to provide the insight into the properties of the $MA(a)$ model.

\begin{proposition}{Stationarity of $\bf MA(q)$}{stationary_of_ma}
    The moving average model \textbf{MA(q)} is stationary. Specifically, for a time-series $X_t$ that follows the ${\bf MA(q)}$ model, we have:
    \begin{itemize}
        \item $\mathbb{E}[X_t] = 0$.
        \item $\gamma(k) = \Bigg( \theta_k + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j \Bigg)\sigma^2$.
    \end{itemize}
\end{proposition}

\begin{proof*}[Proposition \ref{prop:stationary_of_ma}]
    To calculate the mean of $X_t$, we have:
    \begin{align*}
        \mathbb{E}[X_t] &= \mathbb{E}\Bigg[
            Z_t + \sum_{j=1}^q \theta_jZ_{t-j} 
        \Bigg] = \mathbb{E}[Z_t] + \sum_{j=1}^q\theta_j \mathbb{E}[Z_{t-j}] = 0
    \end{align*}

    \noindent To calculate the covariance, we have:
    \begin{align*}
        \gamma(k) &= Cov\Bigg( 
            Z_t + \sum_{i=1}^q \theta_i Z_{t-i}, Z_{t-k} + \sum_{j=1}^q \theta_j Z_{t-k-j}    
        \Bigg) \\
        &= \underbrace{Cov(Z_t, Z_{t-k})}_{=0} + \underbrace{\sum_{j=1}^q \theta_j Cov(Z_t, Z_{t-k-j})}_{=0} + \sum_{i=1}^q \theta_i Cov(Z_{t-i}, Z_{t-k}) \\ &+ \sum_{i=1}^q\sum_{j=1}^q \theta_i\theta_j Cov(Z_{t-i}, Z_{t-k-j})
    \end{align*}

    \noindent When $k > q$, we know that $Cov(Z_{t-i}, Z_{t-k-j})=0$ and $Cov(Z_{t-i}, Z_{t-k})=0$ for all $1 \le i, j \le q$. Therefore, suppose that $1\le q\le k$, we have:

    \begin{align*}
        \gamma(k) &= \sum_{i=1}^q \theta_i Cov(Z_{t-i}, Z_{t-k}) + \sum_{i=1}^q\sum_{j=1}^q \theta_i\theta_j Cov(Z_{t-i}, Z_{t-k-j}) \\
        &= \theta_k Cov(Z_{t-k}, Z_{t-k}) + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j Cov(Z_{t-k-j}, Z_{t-k-j}) \\
        &= \sigma^2\Bigg( 
            \theta_k  + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j
        \Bigg)
    \end{align*}

    \noindent \textbf{Remark} : When we set $k=0$, we obtain the variance and the correlation functions for $X_t$:
    \begin{align*}
        Var(X_t) &= \sigma^2 \Bigg( 1 + \sum_{j=1}^q \theta_j^2 \Bigg) \\
        \implies 
        \rho(k) &= \frac{\theta_k  + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j}{1+\sum_{j=1}^q \theta_j^2 }, \ \ 1 < k \le q
    \end{align*}
\end{proof*}

\begin{definition}[${\bf MA(\infty)}$ model]
    Assume that $Z_t\sim WN(0, \sigma^2)$ and $\psi_0=1$. A time-series $X_t$ following the $\bf MA(\infty)$ model is defined as:
    \begin{align*}
        X_t = \sum_{j=0}^\infty \psi_j Z_{t-j}, \ \ \sum_{j=0}|\psi_j| < \infty
    \end{align*}

    \noindent $X_t$ is called a \textbf{Generalized linear process} or an $\bf MA(\infty)$ process. 
\end{definition}

\subsection{Autoregressive - ${\bf AR(p)}$ model}

