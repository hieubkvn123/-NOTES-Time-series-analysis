\newpage
\section{Stationary time-series}
\subsection{Strict vs. Weak stationarity}

\begin{definition}[Mean, covariance and variance functions]
    Given a time-series $(X_t)_{t\in T}$ (where $T$ is a time domain), we have the following functions:
    \begin{itemize}
        \item Mean : $\mu(t)=\mathbb{E}[X_t], \ t\in T$.
        \item Covariance : $\gamma(t, s) = Cov(X_t, X_s), \ t, s\in T$.
        \item Variance : $\sigma^2(t) = \gamma(t,t)= Var(X_t), \ t\in T$.
    \end{itemize}
\end{definition}


\begin{definition}[Strict stationarity]
    A time-series is said to be \textbf{strictly stationary} if, $\forall n \in \mathbb{Z}_+, h \in \mathbb{N}$, we have:
    \begin{align*}
        (X_1, \dots, X_n) \stackrel{d}{\simeq} (X_{1+h}, \dots, X_{n+h}) 
    \end{align*}

    \noindent Meaning the two tuples are equal in distribution.
\end{definition}

\noindent \textbf{Remark} : Note that in practice, strict stationarity is often hard to achieve. Hence, we have some relaxed conditions for stationarity.

\begin{definition}[Weak stationarity]
    A time-series $(X_t)_{t\ge0}$ is said to be \textbf{weakly stationary} if the following two conditions are met:
    \begin{itemize}
        \item $\mu(t)$ does not depend on $t$.
        \item $\gamma(t, h)$ does not depend on $t$ for all $h\in\mathbb{N}$.
    \end{itemize}
\end{definition}

\subsection{Moving Average - $\bf MA(q)$ model}
\begin{definition}[White noise]
    A process $(Z_t)_{t\in T}$ is called \textbf{white noise} if it satisfies:
    \begin{itemize}
        \item $Cov(Z_t, Z_s)$ for all $s\ne t$.
        \item $Var(Z_t) = \sigma^2, \ \forall t\in T$.
    \end{itemize}

    \noindent We denote that $Z_t\sim WN(0, \sigma^2)$.
\end{definition}

\begin{definition}[$\bf MA(q)$ model]
    The moving average of order $\bf q$ is defined as followed:
    \begin{align*}
        X_t = Z_t + \sum_{j=1}^q \theta_j Z_{t-j}, \ Z_t\sim WN(0,\sigma^2)
    \end{align*}
\end{definition}

\textbf{Remark} : The following propositions are proven to provide the insight into the properties of the $MA(a)$ model.

\begin{proposition}{Stationarity of $\bf MA(q)$}{stationary_of_ma}
    The moving average model \textbf{MA(q)} is stationary. Specifically, for a time-series $X_t$ that follows the ${\bf MA(q)}$ model, we have:
    \begin{itemize}
        \item $\mathbb{E}[X_t] = 0$.
        \item $\gamma(k) = \Bigg( \theta_k + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j \Bigg)\sigma^2$.
    \end{itemize}
\end{proposition}

\begin{proof*}[Proposition \ref{prop:stationary_of_ma}]
    To calculate the mean of $X_t$, we have:
    \begin{align*}
        \mathbb{E}[X_t] &= \mathbb{E}\Bigg[
            Z_t + \sum_{j=1}^q \theta_jZ_{t-j} 
        \Bigg] = \mathbb{E}[Z_t] + \sum_{j=1}^q\theta_j \mathbb{E}[Z_{t-j}] = 0
    \end{align*}

    \noindent To calculate the covariance, we have:
    \begin{align*}
        \gamma(k) &= Cov\Bigg( 
            Z_t + \sum_{i=1}^q \theta_i Z_{t-i}, Z_{t-k} + \sum_{j=1}^q \theta_j Z_{t-k-j}    
        \Bigg) \\
        &= \underbrace{Cov(Z_t, Z_{t-k})}_{=0} + \underbrace{\sum_{j=1}^q \theta_j Cov(Z_t, Z_{t-k-j})}_{=0} + \sum_{i=1}^q \theta_i Cov(Z_{t-i}, Z_{t-k}) \\ &+ \sum_{i=1}^q\sum_{j=1}^q \theta_i\theta_j Cov(Z_{t-i}, Z_{t-k-j})
    \end{align*}

    \noindent When $k > q$, we know that $Cov(Z_{t-i}, Z_{t-k-j})=0$ and $Cov(Z_{t-i}, Z_{t-k})=0$ for all $1 \le i, j \le q$. Therefore, suppose that $1\le q\le k$, we have:

    \begin{align*}
        \gamma(k) &= \sum_{i=1}^q \theta_i Cov(Z_{t-i}, Z_{t-k}) + \sum_{i=1}^q\sum_{j=1}^q \theta_i\theta_j Cov(Z_{t-i}, Z_{t-k-j}) \\
        &= \theta_k Cov(Z_{t-k}, Z_{t-k}) + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j Cov(Z_{t-k-j}, Z_{t-k-j}) \\
        &= \sigma^2\Bigg( 
            \theta_k  + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j
        \Bigg)
    \end{align*}

    \noindent \textbf{Remark} : When we set $k=0$, we obtain the variance and the correlation functions for $X_t$:
    \begin{align*}
        Var(X_t) &= \sigma^2 \Bigg( 1 + \sum_{j=1}^q \theta_j^2 \Bigg) \\
        \implies 
        \rho(k) &= \frac{\theta_k  + \sum_{j=1}^{q-k}\sum_{i=k+j}^q \theta_i \theta_j}{1+\sum_{j=1}^q \theta_j^2 }, \ \ 1 < k \le q
    \end{align*}
\end{proof*}

\begin{definition}[${\bf MA(\infty)}$ model]
    Assume that $Z_t\sim WN(0, \sigma^2)$ and $\psi_0=1$. A time-series $X_t$ following the $\bf MA(\infty)$ model is defined as:
    \begin{align*}
        X_t = \sum_{j=0}^\infty \psi_j Z_{t-j}, \ \ \sum_{j=0}|\psi_j| < \infty
    \end{align*}

    \noindent $X_t$ is called a \textbf{Generalized linear process} or an $\bf MA(\infty)$ process. 
\end{definition}

\subsection{Autoregressive - ${\bf AR(p)}$ model}
\begin{definition}[${\bf AR(p)}$ model]
    The autoregressive model of order $\bf p$ is defined as followed:
    \begin{align*}
        X_{t} = Z_t + \sum_{i=1}^p \phi_i X_{t-i}
    \end{align*}

    \noindent Defining the $p^{th}$ degree polynomial:
    \begin{align*}
        \phi(z) = 1 - \sum_{i=1}^p \phi_i z^i
    \end{align*}

    \noindent We can re-write the $\bf AR(p)$ model as followed:
    \begin{align*}
        Z_t = \phi(B)X_t, \ \ B^i X_t = X_{t-i}
    \end{align*}
\end{definition}

\begin{theorem}{Yule-Walker Equation}{yule_walker_eqn}
    Let $(X_t)_{t\ge0}$ follows the $\bf AR(p)$ model. Let $k\ge1$, the Yule-Walker equations are defined as:
    \begin{align*}
        \rho_k = \phi_1\rho_{k-1} + \dots + \phi_p\rho_{k-p} = \sum_{j=1}^p \phi_j \rho_{k-j}
    \end{align*}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:yule_walker_eqn}]
    We have:
    \begin{align*}
        X_t = Z_t + \sum_{j=1}^p \phi_j X_{t-j}
    \end{align*}

    \noindent Recall the fact that:
    \begin{align*}
        Cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
    \end{align*}
    
    \noindent Hence, we have:
    \begin{align*}
        \gamma_k &= Cov(X_t, X_{t-k}) \\
            &= \E[X_tX_{t-k}] - \E[X_t]\E[X_{t-k}] \ \ \ (\E[X_t]=0, \ \ \forall t \ge 0) \\
            &= \E[X_tX_{t-k}] \\
            &= \E\Bigg[
                Z_tX_{t-k} + \sum_{j=1}^p \phi_j X_{t-j} X_{t-k}    
            \Bigg] \\
            &= \underbrace{\E[Z_tX_{t-k}]}_{=0} + \sum_{j=1}^p \phi_j \E[X_{t-j}X_{t-k}] \\
            &= \sum_{j=1}^p \phi_j \gamma_{k-j}
    \end{align*}

    \noindent Dividing both sides with $\gamma_0$, we have:
    \begin{align*}
        \rho_k = \sum_{j=1}^p \phi_j \rho_{k-j}
    \end{align*}
\end{proof*}

\begin{theorem}{Stationarity of $\bf AR(1)$}{stationarity_of_ar_1}
    $\bf AR(1)$ is stationarity if all the roots of the polynomial $\phi(z)$ are outside the unit circle. Furthermore, the ACF decays exponentially.
\end{theorem}

\begin{proof*}[Theorem \ref{thm:stationarity_of_ar_1}]
    Consider the $\bf AR(1)$ model, we have:
    \begin{align*}
        X_t = \phi_1X_{t-1} + Z_t, \ Z_t \sim WN(0,\sigma^2)
    \end{align*}

    \noindent Then, we have:
    \begin{align*}
        \phi(z) = 1 - \phi_1z
    \end{align*}

    \noindent Hence, we have the root $z = \frac{1}{\phi_1}$. We will show that $X_t$ is stationary if and only if $|\phi_1| < 1$. Hence, $|z| > 1$ and the root lies outside the unit circle. We have:
    \begin{align*}
        X_1 &= \phi_1X_0 + Z_1 \\
        X_2 &= \phi_1(\phi_1X_0 + Z_1) + Z_2 = \phi_1^2X_0 + \phi_1Z_1 + Z_2 \\
        &\dots \\
        X_t &= \phi_1^t X_0 + \sum_{j=1}^t \phi_1^j Z_{t-j}
    \end{align*}

    \noindent Hence, if $|\phi_1| > 1 \implies \E[X_t] = \phi_1^t \E[X_0]$, which is dependent on $t$. On the other hand, $|\phi_1| < 1 \implies \E[X_t]\to0$ as $t\to\infty$. Therefore, $X_t$ is stationary.

    \noindent To prove that the ACF decays exponentially, by the Yule-Walker equations, we have:
    \begin{align*}
        \rho_k &= \phi_1^k \rho_0 = \phi_1^k \\
        \implies |\rho_k| &= |\phi_1|^k = e^{k\log|\phi_1|}
    \end{align*}

    \noindent Hence, when $|\phi_1|< 1$, $|\rho_k|$ decays exponentially.
\end{proof*}

\begin{theorem}{Stationarity of $AR(p)$}{stationarity_of_ar_p}
    $\bf AR(p)$ is stationary if all the roots of the polynomial $\phi(z)$ are outside the unit circle.
\end{theorem}

\begin{proof*}[Theorem \ref{thm:stationarity_of_ar_p}]
    We will revise the proof for this theorem later.
\end{proof*}
